{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Backpropagation**\n",
        "\n",
        "Backpropagation (short for *backward propagation of errors*) is the **learning algorithm** used to train neural networks.\n",
        "\n",
        "It answers one big question:\n",
        "\n",
        "> “Given my network’s prediction and the true label, how do I adjust **each weight and bias** so that the next prediction is a little bit better?”\n",
        "\n",
        "It works by:\n",
        "\n",
        "1. Running a **forward pass** to compute the output and loss.\n",
        "2. Running a **backward pass** to compute how much each weight contributed to the error.\n",
        "3. Updating weights using **gradient descent**.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Why We Need It**\n",
        "\n",
        "In a neural network, weights are deeply connected across layers:\n",
        "\n",
        "* The output layer depends on the hidden layer.\n",
        "* The hidden layer depends on the previous layer.\n",
        "* Changing one weight affects many downstream activations.\n",
        "\n",
        "To train such a network, we need to know:\n",
        "\n",
        "* **How sensitive** the loss is to each weight.\n",
        "* That’s where **gradients** come in — backprop efficiently computes them using the **chain rule of calculus**.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. The Core Idea**\n",
        "\n",
        "Let’s say we have a simple 2-layer network:\n",
        "\n",
        "**Forward pass:**\n",
        "\n",
        "1. Hidden layer pre-activation:\n",
        "\n",
        "   $$\n",
        "   z^{[1]} = W^{[1]}x + b^{[1]}\n",
        "   $$\n",
        "2. Hidden layer activation:\n",
        "\n",
        "   $$\n",
        "   a^{[1]} = f(z^{[1]})\n",
        "   $$\n",
        "3. Output pre-activation:\n",
        "\n",
        "   $$\n",
        "   z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}\n",
        "   $$\n",
        "4. Output activation (prediction):\n",
        "\n",
        "   $$\n",
        "   a^{[2]} = g(z^{[2]})\n",
        "   $$\n",
        "5. Loss:\n",
        "\n",
        "   $$\n",
        "   L = \\text{loss}(y, a^{[2]})\n",
        "   $$\n",
        "\n",
        "---\n",
        "\n",
        "**Backward pass:**\n",
        "We want $\\frac{\\partial L}{\\partial W^{[1]}}, \\frac{\\partial L}{\\partial b^{[1]}}, \\frac{\\partial L}{\\partial W^{[2]}}, \\frac{\\partial L}{\\partial b^{[2]}}$.\n",
        "\n",
        "We apply the **chain rule**:\n",
        "\n",
        "1. **Output layer**:\n",
        "\n",
        "   $$\n",
        "   \\delta^{[2]} = \\frac{\\partial L}{\\partial z^{[2]}} = a^{[2]} - y\n",
        "   $$\n",
        "\n",
        "   $$\n",
        "   \\frac{\\partial L}{\\partial W^{[2]}} = a^{[1]} \\cdot (\\delta^{[2]})^\\top\n",
        "   $$\n",
        "\n",
        "   $$\n",
        "   \\frac{\\partial L}{\\partial b^{[2]}} = \\delta^{[2]}\n",
        "   $$\n",
        "\n",
        "2. **Hidden layer**:\n",
        "   Backpropagate error to hidden layer:\n",
        "\n",
        "   $$\n",
        "   \\delta^{[1]} = (W^{[2]} \\delta^{[2]}) \\odot f'(z^{[1]})\n",
        "   $$\n",
        "\n",
        "   $$\n",
        "   \\frac{\\partial L}{\\partial W^{[1]}} = x \\cdot (\\delta^{[1]})^\\top\n",
        "   $$\n",
        "\n",
        "   $$\n",
        "   \\frac{\\partial L}{\\partial b^{[1]}} = \\delta^{[1]}\n",
        "   $$\n",
        "\n",
        "Here:\n",
        "\n",
        "* $\\odot$ means element-wise multiplication.\n",
        "* $f'(z)$ is the derivative of the activation function.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Why the Name \"Backpropagation\"**\n",
        "\n",
        "* We **propagate forward** to get predictions.\n",
        "* We **propagate backward** the error signal layer by layer, computing gradients.\n",
        "* The process is “backward” because we start from the output layer and go in reverse to the input.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. A Tiny Intuition Example**\n",
        "\n",
        "Imagine predicting house prices with 2 features:\n",
        "\n",
        "* If the prediction is too high, we want to **reduce** the weights that led to that output.\n",
        "* If the prediction is too low, we want to **increase** them.\n",
        "* Backprop tells us **exactly how much** to tweak each weight.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Connection to Gradient Descent**\n",
        "\n",
        "Once we have all partial derivatives:\n",
        "\n",
        "$$\n",
        "W := W - \\eta \\frac{\\partial L}{\\partial W}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b := b - \\eta \\frac{\\partial L}{\\partial b}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $\\eta$ = learning rate\n",
        "* $\\frac{\\partial L}{\\partial W}$ = gradient (slope of loss surface)"
      ],
      "metadata": {
        "id": "SkY5oSVk3s6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid and derivative\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "# Data\n",
        "x = 0.5\n",
        "y_true = 1\n",
        "\n",
        "# Parameters\n",
        "W1, b1 = 0.4, 0.3\n",
        "W2, b2 = 0.7, 0.2\n",
        "lr = 0.1\n",
        "\n",
        "# Forward pass\n",
        "z1 = W1 * x + b1\n",
        "a1 = sigmoid(z1)\n",
        "\n",
        "z2 = W2 * a1 + b2\n",
        "a2 = sigmoid(z2)\n",
        "\n",
        "loss = 0.5 * (y_true - a2) ** 2\n",
        "\n",
        "print(f\"Forward pass -> a2: {a2:.6f}, Loss: {loss:.6f}\")\n",
        "\n",
        "# Backward pass\n",
        "delta2 = (a2 - y_true) * sigmoid_derivative(z2)\n",
        "dW2 = a1 * delta2\n",
        "db2 = delta2\n",
        "\n",
        "delta1 = (W2 * delta2) * sigmoid_derivative(z1)\n",
        "dW1 = x * delta1\n",
        "db1 = delta1\n",
        "\n",
        "# Update parameters\n",
        "W1 -= lr * dW1\n",
        "b1 -= lr * db1\n",
        "W2 -= lr * dW2\n",
        "b2 -= lr * db2\n",
        "\n",
        "print(f\"Updated parameters -> W1: {W1}, b1: {b1}, W2: {W2}, b2: {b2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKggr_e_39Uu",
        "outputId": "7bef650e-0711-44d1-f0ba-400659c54516"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward pass -> a2: 0.653786, Loss: 0.059932\n",
            "Updated parameters -> W1: 0.4006445672678901, b1: 0.3012891345357802, W2: 0.7048779400938787, b2: 0.20783656031705777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pdoKeqGj4IPf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}